# === Imports ===
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# === 1. Charger un mod√®le d'embedding open-source ===
embedder = SentenceTransformer("all-MiniLM-L6-v2")  # rapide & efficace

# === 2. Tes documents sources (√† indexer dans FAISS) ===
docs = [
    "Les pandas sont des mammif√®res herbivores originaires de Chine.",
    "Python est un langage de programmation populaire pour la data science.",
    "FAISS est une librairie de Facebook pour la recherche vectorielle rapide.",
    "DeepSeek est un mod√®le open-source pour coder et g√©n√©rer du texte.",
]

# === 3. Cr√©er les embeddings pour les documents ===
doc_embeddings = embedder.encode(docs, convert_to_numpy=True)

# === 4. Initialiser l'index FAISS ===
dimension = doc_embeddings.shape[1]  # 384 dimensions
index = faiss.IndexFlatL2(dimension)  # index simple bas√© sur la distance L2
index.add(doc_embeddings)  # ajoute les vecteurs √† l'index

# === 5. Question de l'utilisateur ===
user_query = "C'est quoi FAISS et √† quoi √ßa sert ?"

# === 6. Embedding de la question ===
query_embedding = embedder.encode([user_query])

# === 7. Recherche dans FAISS (top 2 r√©sultats) ===
top_k = 2
D, I = index.search(np.array(query_embedding), top_k)

# === 8. R√©cup√©rer les documents les plus pertinents ===
retrieved_docs = [docs[i] for i in I[0]]

# === 9. Construire le prompt pour DeepSeek ===
context = "\n".join(retrieved_docs)
final_prompt = f"""Tu es un assistant expert. Voici des infos utiles :\n{context}\n\nR√©pond √† la question suivante : {user_query}"""

# === 10. Charger DeepSeek pour la g√©n√©ration ===
model_name = "deepseek-ai/deepseek-coder-1.3b-base"  # open source !
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# === 11. G√©n√©ration avec pipeline Hugging Face ===
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)
output = generator(final_prompt, max_new_tokens=150, do_sample=False)[0]["generated_text"]

# === 12. Afficher la r√©ponse g√©n√©r√©e ===
print("üîç Question:", user_query)
print("\nüß† Connaissances trouv√©es :")
print(context)
print("\n‚úçÔ∏è R√©ponse g√©n√©r√©e :")
print(output.split(final_prompt)[-1].strip())
